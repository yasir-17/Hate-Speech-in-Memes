{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preserve_unused_tokens=False']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from absl import flags\n",
    "sys.argv=['preserve_unused_tokens=False']\n",
    "flags.FLAGS(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in c:\\users\\yasir\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\yasir\\anaconda3\\lib\\site-packages (from bert-tensorflow) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading tokenization script created by the Google\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8313, 100, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_feature = np.load('Emb_feature/new_test_features/.npy')\n",
    "np.shape(img_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the picture man in black and white cap is holding up sign . And the text says: its their character not their color that matters \n",
      "\n",
      "\n",
      "8313\n"
     ]
    }
   ],
   "source": [
    "FMS = pd.read_csv(\"FMS_final.csv\")\n",
    "FMS['textNdesc'] = 'In the picture '+ FMS.gen_caption + ' And the text says: ' + FMS.text\n",
    "print(FMS.textNdesc[0], '\\n\\n')\n",
    "print(len(FMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    image_input = tf.keras.Input(shape=(100,1024), batch_size=None, name=\"image_input\")\n",
    "    image_flatten = tf.keras.layers.Flatten()(image_input)\n",
    "    image_dense_1 = tf.keras.layers.Dense(1024, activation = tf.nn.relu, kernel_initializer = tf.keras.initializers.he_uniform(seed=54))(image_flatten)\n",
    "    image_dense_2 = tf.keras.layers.Dense(1024, activation = tf.nn.relu, kernel_initializer = tf.keras.initializers.he_uniform(seed=32))(image_dense_1)\n",
    "\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "\n",
    "    print(np.shape(sequence_output))\n",
    "    print(np.shape(clf_output))\n",
    "\n",
    "    image_question = tf.keras.layers.Multiply()([image_dense_2, clf_output])\n",
    "    image_question_dense_1 = tf.keras.layers.Dense(1000, activation = tf.nn.relu, kernel_initializer = tf.keras.initializers.he_uniform(seed=19))(image_question)\n",
    "    image_question_dense_2 = tf.keras.layers.Dense(1000, activation = tf.nn.relu, kernel_initializer = tf.keras.initializers.he_uniform(seed=28))(image_question_dense_1)\n",
    "    output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, kernel_initializer = tf.keras.initializers.glorot_normal(seed=15))(image_question_dense_2)\n",
    "    \n",
    "    # out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids, image_input], outputs=output)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(FMS.textNdesc.values, tokenizer, max_len=160)\n",
    "train_labels = FMS.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8313, 160)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 160, 1024)\n",
      "(None, 1024)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 100, 1024)]  0           []                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 102400)       0           ['image_input[0][0]']            \n",
      "                                                                                                  \n",
      " input_word_ids (InputLayer)    [(None, 160)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 160)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 160)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         104858624   ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       [(None, 1024),       335141889   ['input_word_ids[0][0]',         \n",
      "                                 (None, 160, 1024)]               'input_mask[0][0]',             \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         1049600     ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 1024)        0           ['keras_layer[0][1]']            \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 1024)         0           ['dense_1[0][0]',                \n",
      "                                                                  'tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1000)         1025000     ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1000)         1001000     ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            1001        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 443,077,114\n",
      "Trainable params: 107,935,225\n",
      "Non-trainable params: 335,141,889\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yasir\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1453s 129s/step - loss: 0.9476 - accuracy: 0.6276 - auc: 0.4819 - val_loss: 0.6976 - val_accuracy: 0.5938 - val_auc: 0.5521\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1391s 125s/step - loss: 0.6844 - accuracy: 0.6068 - auc: 0.5422 - val_loss: 0.6968 - val_accuracy: 0.5674 - val_auc: 0.6113\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1432s 129s/step - loss: 0.6747 - accuracy: 0.6354 - auc: 0.6000 - val_loss: 0.6892 - val_accuracy: 0.5958 - val_auc: 0.6598\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1418s 127s/step - loss: 0.6702 - accuracy: 0.6276 - auc: 0.6223 - val_loss: 0.6809 - val_accuracy: 0.5966 - val_auc: 0.6684\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1383s 124s/step - loss: 0.6504 - accuracy: 0.6693 - auc: 0.5705 - val_loss: 0.6292 - val_accuracy: 0.6544 - val_auc: 0.6759\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1368s 123s/step - loss: 0.6669 - accuracy: 0.6458 - auc: 0.6291 - val_loss: 0.6689 - val_accuracy: 0.6283 - val_auc: 0.6417\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1380s 124s/step - loss: 0.6002 - accuracy: 0.7135 - auc: 0.6284 - val_loss: 0.6432 - val_accuracy: 0.6468 - val_auc: 0.6686\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1387s 125s/step - loss: 0.7043 - accuracy: 0.6276 - auc: 0.5319 - val_loss: 0.6420 - val_accuracy: 0.6423 - val_auc: 0.6670\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1370s 123s/step - loss: 0.6493 - accuracy: 0.6432 - auc: 0.6145 - val_loss: 0.6931 - val_accuracy: 0.6195 - val_auc: 0.6719\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1355s 122s/step - loss: 0.6242 - accuracy: 0.6693 - auc: 0.6423 - val_loss: 0.6726 - val_accuracy: 0.6315 - val_auc: 0.6764\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1349s 121s/step - loss: 0.6533 - accuracy: 0.6250 - auc: 0.6424 - val_loss: 0.6311 - val_accuracy: 0.6692 - val_auc: 0.6865\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1355s 122s/step - loss: 0.6013 - accuracy: 0.6953 - auc: 0.6616 - val_loss: 0.6332 - val_accuracy: 0.6648 - val_auc: 0.6842\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1352s 121s/step - loss: 0.6213 - accuracy: 0.6771 - auc: 0.6879 - val_loss: 0.6532 - val_accuracy: 0.6484 - val_auc: 0.6674\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1355s 122s/step - loss: 0.6100 - accuracy: 0.6641 - auc: 0.6547 - val_loss: 0.6587 - val_accuracy: 0.6247 - val_auc: 0.6697\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1384s 124s/step - loss: 0.6250 - accuracy: 0.6536 - auc: 0.6415 - val_loss: 0.6203 - val_accuracy: 0.6640 - val_auc: 0.6961\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1352s 122s/step - loss: 0.6726 - accuracy: 0.6253 - auc: 0.5977 - val_loss: 0.6557 - val_accuracy: 0.6387 - val_auc: 0.6816\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1352s 121s/step - loss: 0.6359 - accuracy: 0.6536 - auc: 0.6575 - val_loss: 0.6533 - val_accuracy: 0.6403 - val_auc: 0.6902\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1380s 124s/step - loss: 0.6311 - accuracy: 0.6615 - auc: 0.6525 - val_loss: 0.6178 - val_accuracy: 0.6800 - val_auc: 0.7012\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1359s 122s/step - loss: 0.5860 - accuracy: 0.6927 - auc: 0.6917 - val_loss: 0.6348 - val_accuracy: 0.6696 - val_auc: 0.6973\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1353s 122s/step - loss: 0.6829 - accuracy: 0.6224 - auc: 0.6123 - val_loss: 0.7197 - val_accuracy: 0.6219 - val_auc: 0.6872\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1357s 122s/step - loss: 0.6145 - accuracy: 0.6875 - auc: 0.6695 - val_loss: 0.6195 - val_accuracy: 0.6816 - val_auc: 0.6954\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1356s 122s/step - loss: 0.5947 - accuracy: 0.7083 - auc: 0.6748 - val_loss: 0.6278 - val_accuracy: 0.6688 - val_auc: 0.6944\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1353s 122s/step - loss: 0.5352 - accuracy: 0.7500 - auc: 0.7566 - val_loss: 0.6494 - val_accuracy: 0.6576 - val_auc: 0.6923\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1355s 122s/step - loss: 0.6445 - accuracy: 0.6667 - auc: 0.6198 - val_loss: 0.6383 - val_accuracy: 0.6712 - val_auc: 0.7001\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1358s 122s/step - loss: 0.5656 - accuracy: 0.7161 - auc: 0.7181 - val_loss: 0.6201 - val_accuracy: 0.6740 - val_auc: 0.7075\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1355s 122s/step - loss: 0.6143 - accuracy: 0.6823 - auc: 0.6774 - val_loss: 0.6214 - val_accuracy: 0.6620 - val_auc: 0.6953\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1353s 122s/step - loss: 0.6127 - accuracy: 0.6536 - auc: 0.6788 - val_loss: 0.6210 - val_accuracy: 0.6708 - val_auc: 0.6990\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1390s 125s/step - loss: 0.5755 - accuracy: 0.6927 - auc: 0.7063 - val_loss: 0.6174 - val_accuracy: 0.6732 - val_auc: 0.6999\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1359s 122s/step - loss: 0.6526 - accuracy: 0.6536 - auc: 0.6322 - val_loss: 0.6964 - val_accuracy: 0.6247 - val_auc: 0.6924\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1385s 124s/step - loss: 0.6482 - accuracy: 0.6536 - auc: 0.5921 - val_loss: 0.6850 - val_accuracy: 0.6299 - val_auc: 0.6853\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1406s 126s/step - loss: 0.6079 - accuracy: 0.6939 - auc: 0.6796 - val_loss: 0.6134 - val_accuracy: 0.6716 - val_auc: 0.6969\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1341s 121s/step - loss: 0.5879 - accuracy: 0.6979 - auc: 0.7118 - val_loss: 0.6183 - val_accuracy: 0.6660 - val_auc: 0.6967\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1376s 124s/step - loss: 0.6297 - accuracy: 0.6536 - auc: 0.6455 - val_loss: 0.6219 - val_accuracy: 0.6624 - val_auc: 0.6972\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1381s 124s/step - loss: 0.6031 - accuracy: 0.6901 - auc: 0.6541 - val_loss: 0.6120 - val_accuracy: 0.6712 - val_auc: 0.7076\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1377s 124s/step - loss: 0.5698 - accuracy: 0.6953 - auc: 0.6898 - val_loss: 0.6525 - val_accuracy: 0.6508 - val_auc: 0.6897\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1347s 121s/step - loss: 0.5481 - accuracy: 0.7188 - auc: 0.7474 - val_loss: 0.6256 - val_accuracy: 0.6704 - val_auc: 0.7037\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1350s 121s/step - loss: 0.5557 - accuracy: 0.7448 - auc: 0.7120 - val_loss: 0.6274 - val_accuracy: 0.6824 - val_auc: 0.7154\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1354s 122s/step - loss: 0.6146 - accuracy: 0.6745 - auc: 0.6897 - val_loss: 0.6154 - val_accuracy: 0.6656 - val_auc: 0.7068\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1345s 121s/step - loss: 0.5579 - accuracy: 0.7266 - auc: 0.7389 - val_loss: 0.6204 - val_accuracy: 0.6740 - val_auc: 0.7018\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1383s 124s/step - loss: 0.6058 - accuracy: 0.6823 - auc: 0.6731 - val_loss: 0.6107 - val_accuracy: 0.6840 - val_auc: 0.7090\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1394s 125s/step - loss: 0.6419 - accuracy: 0.6484 - auc: 0.6526 - val_loss: 0.6146 - val_accuracy: 0.6752 - val_auc: 0.7050\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1365s 123s/step - loss: 0.5866 - accuracy: 0.7005 - auc: 0.6980 - val_loss: 0.6277 - val_accuracy: 0.6604 - val_auc: 0.6973\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1351s 121s/step - loss: 0.5504 - accuracy: 0.7214 - auc: 0.7193 - val_loss: 0.6597 - val_accuracy: 0.6496 - val_auc: 0.7079\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1377s 124s/step - loss: 0.6664 - accuracy: 0.6432 - auc: 0.6578 - val_loss: 0.6048 - val_accuracy: 0.6868 - val_auc: 0.7159\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1352s 122s/step - loss: 0.6359 - accuracy: 0.6693 - auc: 0.6656 - val_loss: 0.6328 - val_accuracy: 0.6604 - val_auc: 0.6885\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1379s 124s/step - loss: 0.6318 - accuracy: 0.6491 - auc: 0.6510 - val_loss: 0.6209 - val_accuracy: 0.6684 - val_auc: 0.6933\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1385s 124s/step - loss: 0.5510 - accuracy: 0.7266 - auc: 0.7110 - val_loss: 0.7214 - val_accuracy: 0.6219 - val_auc: 0.6687\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1366s 123s/step - loss: 0.6212 - accuracy: 0.6693 - auc: 0.6349 - val_loss: 0.6361 - val_accuracy: 0.6395 - val_auc: 0.6663\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1405s 126s/step - loss: 0.6259 - accuracy: 0.6745 - auc: 0.6426 - val_loss: 0.6315 - val_accuracy: 0.6628 - val_auc: 0.6856\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1403s 126s/step - loss: 0.5728 - accuracy: 0.6875 - auc: 0.7273 - val_loss: 0.6196 - val_accuracy: 0.6708 - val_auc: 0.6938\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1364s 123s/step - loss: 0.5517 - accuracy: 0.7240 - auc: 0.7552 - val_loss: 0.6508 - val_accuracy: 0.6596 - val_auc: 0.7068\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1381s 124s/step - loss: 0.5667 - accuracy: 0.7031 - auc: 0.7120 - val_loss: 0.6102 - val_accuracy: 0.6796 - val_auc: 0.7095\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1355s 122s/step - loss: 0.5616 - accuracy: 0.7057 - auc: 0.7452 - val_loss: 0.6141 - val_accuracy: 0.6901 - val_auc: 0.7112\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1357s 122s/step - loss: 0.6253 - accuracy: 0.6745 - auc: 0.6526 - val_loss: 0.6387 - val_accuracy: 0.6608 - val_auc: 0.7044\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1361s 122s/step - loss: 0.5452 - accuracy: 0.7370 - auc: 0.7527 - val_loss: 0.6167 - val_accuracy: 0.6632 - val_auc: 0.7016\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1367s 123s/step - loss: 0.5785 - accuracy: 0.7109 - auc: 0.7080 - val_loss: 0.6378 - val_accuracy: 0.6600 - val_auc: 0.7009\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1346s 121s/step - loss: 0.6143 - accuracy: 0.6641 - auc: 0.6746 - val_loss: 0.6124 - val_accuracy: 0.6740 - val_auc: 0.7037\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1345s 121s/step - loss: 0.5752 - accuracy: 0.7031 - auc: 0.7368 - val_loss: 0.6100 - val_accuracy: 0.6780 - val_auc: 0.7072\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1344s 121s/step - loss: 0.6004 - accuracy: 0.7031 - auc: 0.6763 - val_loss: 0.6104 - val_accuracy: 0.6808 - val_auc: 0.7076\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1348s 121s/step - loss: 0.6215 - accuracy: 0.6589 - auc: 0.6661 - val_loss: 0.6425 - val_accuracy: 0.6620 - val_auc: 0.6992\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1344s 121s/step - loss: 0.6098 - accuracy: 0.6834 - auc: 0.6733 - val_loss: 0.6239 - val_accuracy: 0.6624 - val_auc: 0.6987\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5353 - accuracy: 0.7526 - auc: 0.7751 "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "checkpoint = ModelCheckpoint('model4.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    [train_input, img_feature], train_labels,\n",
    "    validation_split=0.3,\n",
    "    epochs=100 ,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=32,\n",
    "    steps_per_epoch=12   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f7660018a1f48e52e1285a6cd46b08bf9e759a1146999e72bfa769101423a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
