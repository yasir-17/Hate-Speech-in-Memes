{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c492cd-0225-4d52-b861-4589efccedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import gc\n",
    "import random\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3014c2-87de-4713-a715-3c3f30d97824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                                    caption  \n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .  \n",
       "1                                     A girl going into a wooden building .  \n",
       "2                          A little girl climbing into a wooden playhouse .  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"resnet/captions.txt\", sep=',')\n",
    "print(len(df))\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b97bfd0e-8da5-47d7-974e-a9e7309295f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_char_word(word_list):\n",
    "    lst = []\n",
    "    for word in word_list:\n",
    "        if len(word)>1:\n",
    "            lst.append(word)\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2b8cc7-775b-4b4c-9592-5c2d8c9c4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_caption'] = df['caption'].apply(lambda caption : ['<start>'] + [word.lower() if word.isalpha() else '' for word in caption.split(\" \")] + ['<end>'])\n",
    "df['cleaned_caption']  = df['cleaned_caption'].apply(lambda x : remove_single_char_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630b5952-0d2c-4035-be59-b6bf87b5cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "df['seq_len'] = df['cleaned_caption'].apply(lambda x : len(x))\n",
    "max_seq_len = df['seq_len'].max()\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a02f7f-9770-47f4-9c27-1002e344e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['seq_len'], axis = 1, inplace = True)\n",
    "df['cleaned_caption'] = df['cleaned_caption'].apply(lambda caption : caption + ['<pad>']*(max_seq_len-len(caption)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae28a1a-e58a-4941-b022-63b9af50449e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>cleaned_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "      <td>[&lt;start&gt;, child, in, pink, dress, is, climbing, up, set, of, stairs, in, an, entry, way, &lt;end&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>[&lt;start&gt;, girl, going, into, wooden, building, &lt;end&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                                    caption  \\\n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .   \n",
       "1                                     A girl going into a wooden building .   \n",
       "\n",
       "                                                                                                                                                                                                                               cleaned_caption  \n",
       "0                       [<start>, child, in, pink, dress, is, climbing, up, set, of, stairs, in, an, entry, way, <end>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]  \n",
       "1  [<start>, girl, going, into, wooden, building, <end>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef0a0bb-e53d-437f-95cf-d5082d7fc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = df['cleaned_caption'].apply(lambda x : \" \".join(x)).str.cat(sep = ' ').split(' ')\n",
    "word_dict = Counter(word_list)\n",
    "word_dict =  sorted(word_dict, key=word_dict.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d2c3ca-fed2-4492-b2e7-363f7961828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8360\n",
      "['<pad>', '<start>', '<end>', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(len(word_dict))\n",
    "print(word_dict[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3d6d70-2ef5-42e1-9b85-95e4e3b63718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8360\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_dict)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89801cf9-aeb4-4710-9af3-1cec7469632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8360 8360\n"
     ]
    }
   ],
   "source": [
    "index_to_word = {index: word for index, word in enumerate(word_dict)}\n",
    "word_to_index = {word: index for index, word in enumerate(word_dict)}\n",
    "print(len(index_to_word), len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd1c3ef2-9a4c-4a12-a1f8-2087505af17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_seq']  = df['cleaned_caption'].apply(lambda caption : [word_to_index[word] for word in caption] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdb863a4-9c3e-4da7-b3e1-9c4a41f60038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>cleaned_caption</th>\n",
       "      <th>text_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "      <td>[&lt;start&gt;, child, in, pink, dress, is, climbing, up, set, of, stairs, in, an, entry, way, &lt;end&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "      <td>[1, 41, 3, 89, 168, 6, 118, 52, 392, 11, 389, 3, 27, 5075, 690, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>[&lt;start&gt;, girl, going, into, wooden, building, &lt;end&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "      <td>[1, 18, 311, 63, 192, 116, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                                    caption  \\\n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .   \n",
       "1                                     A girl going into a wooden building .   \n",
       "\n",
       "                                                                                                                                                                                                                               cleaned_caption  \\\n",
       "0                       [<start>, child, in, pink, dress, is, climbing, up, set, of, stairs, in, an, entry, way, <end>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]   \n",
       "1  [<start>, girl, going, into, wooden, building, <end>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]   \n",
       "\n",
       "                                                                                                                text_seq  \n",
       "0  [1, 41, 3, 89, 168, 6, 118, 52, 392, 11, 389, 3, 27, 5075, 690, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1            [1, 18, 311, 63, 192, 116, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86a16d3b-5ae3-40f0-ac67-ee928b1b56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = 'image')\n",
    "train = df.iloc[:int(0.9*len(df))]\n",
    "valid = df.iloc[int(0.9*len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad3a183c-be10-48ea-bbdf-ca8f80c43ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36409 7282\n",
      "4046 810\n"
     ]
    }
   ],
   "source": [
    "print(len(train), train['image'].nunique())\n",
    "print(len(valid), valid['image'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e720c463-9320-4082-9e46-efdb568ec5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_train_imgs = train[['image']].drop_duplicates()\n",
    "unq_valid_imgs = valid[['image']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a6e735d-1be6-4914-9856-1d6782953934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class extractImageFeatureResNetDataSet():\n",
    "    def __init__(self, data):\n",
    "        self.data = data \n",
    "        self.scaler = transforms.Resize([224, 224])\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "    def __len__(self):  \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_name = self.data.iloc[idx]['image']\n",
    "        img_loc = 'resnet/Images/'+str(image_name)\n",
    "       \n",
    "        img = Image.open(img_loc)\n",
    "        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n",
    "\n",
    "        return image_name, t_img\n",
    "\n",
    "class extractImageFeatureResNetTestDataSet():\n",
    "    def __init__(self, data):\n",
    "        self.data = data \n",
    "        self.scaler = transforms.Resize([224, 224])\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "    def __len__(self):  \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_name = self.data.iloc[idx]['image']\n",
    "        img_loc = \"Data/img/\"+str(image_name)\n",
    "\n",
    "        img = Image.open(img_loc).convert('RGB')\n",
    "        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n",
    "\n",
    "        return image_name, t_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15fa637f-c0b2-42f5-ac70-de5a1c55eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_train_imgs)\n",
    "train_ImageDataloader_ResNet = DataLoader(train_ImageDataset_ResNet, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4793ac32-fa32-4d39-af84-6534921ac719",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_valid_imgs)\n",
    "valid_ImageDataloader_ResNet = DataLoader(valid_ImageDataset_ResNet, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcdb4a83-f8ad-4e04-8fbe-3d58ea1c826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c576076-902e-4414-bf04-b0aea3d049a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasir\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yasir\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['conv1',\n",
       " 'bn1',\n",
       " 'relu',\n",
       " 'maxpool',\n",
       " 'layer1',\n",
       " 'layer2',\n",
       " 'layer3',\n",
       " 'layer4',\n",
       " 'avgpool',\n",
       " 'fc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True).to(device)\n",
    "resnet18.eval()\n",
    "list(resnet18._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e136986-0e26-4b03-9adf-89d2177c3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "resNet18Layer4 = resnet18._modules.get('layer4').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f1bbbb5-83c6-42f1-9dd1-301d0143d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(t_img):\n",
    "    t_img = Variable(t_img)\n",
    "    my_embedding = torch.zeros(1, 512, 7, 7)\n",
    "    def copy_data(m, i, o):\n",
    "        my_embedding.copy_(o.data)\n",
    "\n",
    "    h = resNet18Layer4.register_forward_hook(copy_data)\n",
    "    resnet18(t_img)\n",
    "\n",
    "    h.remove()\n",
    "    return my_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe552f6c-b5ef-4ef3-8e96-03f5ff5079a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f86da3152247ddab96e473b15c54ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_imgFtr_ResNet_train = {}\n",
    "for image_name, t_img in tqdm(train_ImageDataloader_ResNet):\n",
    "    t_img = t_img.to(device)\n",
    "    embdg = get_vector(t_img)\n",
    "    \n",
    "    extract_imgFtr_ResNet_train[image_name[0]] = embdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8287a552-a560-46fe-bb87-7ae64b26f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipywidgets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de587cc4-2cb8-4fdc-815a-a7d122e43acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./EncodedImageTrainResNet.pkl\", \"wb\")\n",
    "pickle.dump(extract_imgFtr_ResNet_train, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "631e1eae-3b21-4f46-8204-565dd7942643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2ca199045b4cbd81cb74468ba8f837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_imgFtr_ResNet_valid = {}\n",
    "for image_name, t_img in tqdm(valid_ImageDataloader_ResNet):\n",
    "    t_img = t_img.to(device)\n",
    "    embdg = get_vector(t_img)\n",
    " \n",
    "    extract_imgFtr_ResNet_valid[image_name[0]] = embdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44147bf3-91bd-4dc5-a676-62e07e2c0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./EncodedImageValidResNet.pkl\", \"wb\")\n",
    "pickle.dump(extract_imgFtr_ResNet_valid, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "438d9734-57c9-4097-9eea-e60dd1819f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "images = glob.glob(\"Data/img/\" + \"*.png\")\n",
    "images\n",
    "id = []\n",
    "for i in range(len(images)):\n",
    "    id.append(images[i][-9:-4])\n",
    "    images[i] = images[i][-9:]\n",
    "# print(id)   \n",
    "unq_test_imgs = pd.DataFrame({'image': images, 'id':id})\n",
    "unq_test_imgs2 = pd.DataFrame({'image': images})\n",
    "\n",
    "# test = pd.concat([test[:4188], test[4191:4989], test[4995:5299], test[5311:6408], test[6432:6898], test[6946:]])\n",
    "\n",
    "# unq_test_imgs = test[['image']].drop_duplicates()\n",
    "# print(len(unq_train_imgs), len(unq_valid_imgs), len(unq_test_imgs)) \n",
    "\n",
    "test_ImageDataset_ResNet = extractImageFeatureResNetTestDataSet(unq_test_imgs2)\n",
    "test_ImageDataloader_ResNet = DataLoader(test_ImageDataset_ResNet, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "515057bd-9030-4e10-8ac7-c69bdde88fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# images = glob.glob(\"/content/drive/MyDrive/Hate Speech/Data/img/\" + \"*.png\")\n",
    "# images\n",
    "# for i in range(len(images)):\n",
    "#     images[i] = images[i][-9:]\n",
    "# test = pd.DataFrame({'image': images})\n",
    "# test = pd.concat([test[:4188], test[4191:4989], test[4995:5299], test[5311:6408], test[6432:6898], test[6946:]])\n",
    "\n",
    "# unq_test_imgs = test[['image']].drop_duplicates()\n",
    "# print(len(unq_train_imgs), len(unq_valid_imgs), len(unq_test_imgs)) \n",
    "\n",
    "# test_ImageDataset_ResNet = extractImageFeatureResNetTestDataSet(unq_test_imgs)\n",
    "# test_ImageDataloader_ResNet = DataLoader(test_ImageDataset_ResNet, batch_size = 1, shuffle=False)\n",
    "# print(test_ImageDataset_ResNet)\n",
    "# print(test_ImageDataloader_ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bdbaf2d-baed-4c38-933e-fe432ae228f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8291</td>\n",
       "      <td>img/08291.png</td>\n",
       "      <td>1</td>\n",
       "      <td>white people is this a shooting range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46971</td>\n",
       "      <td>img/46971.png</td>\n",
       "      <td>1</td>\n",
       "      <td>bravery at its finest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3745</td>\n",
       "      <td>img/03745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>your order comes to $37.50 and your white privilege discount brings the total to $37.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83745</td>\n",
       "      <td>img/83745.png</td>\n",
       "      <td>1</td>\n",
       "      <td>it is time.. to send these parasites back to the desert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80243</td>\n",
       "      <td>img/80243.png</td>\n",
       "      <td>1</td>\n",
       "      <td>mississippi wind chime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>83675</td>\n",
       "      <td>img/83675.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm gonna be like phelps one day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>37198</td>\n",
       "      <td>img/37198.png</td>\n",
       "      <td>0</td>\n",
       "      <td>when you're so relaxed you can feel yourself go to the next level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>48670</td>\n",
       "      <td>img/48670.png</td>\n",
       "      <td>0</td>\n",
       "      <td>look at this sandwich maker club i found on wish on clearance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>9863</td>\n",
       "      <td>img/09863.png</td>\n",
       "      <td>0</td>\n",
       "      <td>diverse group of women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>97320</td>\n",
       "      <td>img/97320.png</td>\n",
       "      <td>0</td>\n",
       "      <td>\"when your dishwasher is broken so you take it back to walmart to get a new one\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id            img  label  \\\n",
       "0     8291  img/08291.png      1   \n",
       "1    46971  img/46971.png      1   \n",
       "2     3745  img/03745.png      1   \n",
       "3    83745  img/83745.png      1   \n",
       "4    80243  img/80243.png      1   \n",
       "..     ...            ...    ...   \n",
       "495  83675  img/83675.png      0   \n",
       "496  37198  img/37198.png      0   \n",
       "497  48670  img/48670.png      0   \n",
       "498   9863  img/09863.png      0   \n",
       "499  97320  img/97320.png      0   \n",
       "\n",
       "                                                                                        text  \n",
       "0                                                      white people is this a shooting range  \n",
       "1                                                                      bravery at its finest  \n",
       "2    your order comes to $37.50 and your white privilege discount brings the total to $37.50  \n",
       "3                                    it is time.. to send these parasites back to the desert  \n",
       "4                                                                     mississippi wind chime  \n",
       "..                                                                                       ...  \n",
       "495                                                         i'm gonna be like phelps one day  \n",
       "496                        when you're so relaxed you can feel yourself go to the next level  \n",
       "497                            look at this sandwich maker club i found on wish on clearance  \n",
       "498                                                                   diverse group of women  \n",
       "499         \"when your dishwasher is broken so you take it back to walmart to get a new one\"  \n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_json('Data/dev.jsonl', lines=True)\n",
    "df_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcd837b4-2bf8-43f3-89dd-c100000b0138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data/img/97320.png'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "for i in range(0, df_train.shape[0]):\n",
    "    #im = image.load_img(\"Data/\" + df_train[\"img\"][i], target_size=(224,224))\n",
    "    images = glob.glob(\"Data/\" + df_train[\"img\"][i])\n",
    "# for i in range(len(images)):\n",
    "#     images[i] = images[i][-9:]\n",
    "    \n",
    "# test = pd.DataFrame({'image': images})\n",
    "\n",
    "# df_final = pd.concat([df_train, test], axis = 1)\n",
    "# df_final\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4445763f-c2b1-41a2-87c4-0eb1482ac36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image    01235.png\n",
       "id           01235\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unq_test_imgs.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a84288ac-39ff-4c24-a3aa-acc76c908c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b81945d8d184e0aa314936da77460bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_imgFtr_ResNet_test = {}\n",
    "for image_name, t_img in tqdm(test_ImageDataloader_ResNet):\n",
    "    t_img = t_img.to(device)\n",
    "    embdg = get_vector(t_img)\n",
    "\n",
    "    extract_imgFtr_ResNet_test[image_name[0]] = embdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "810857c7-fdc2-431d-8846-8e1601edb1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./EncodedImageTestResNet.pkl\", \"wb\")\n",
    "pickle.dump(extract_imgFtr_ResNet_test, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce7ec985-ed6a-4316-b9a8-874d00643dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickerDataSetResnet():\n",
    "    def __init__(self, data, pkl_file):\n",
    "        self.data = data\n",
    "        self.encodedImgs = pd.read_pickle(pkl_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        caption_seq = self.data.iloc[idx]['text_seq']\n",
    "        target_seq = caption_seq[1:]+[0]\n",
    "\n",
    "        image_name = self.data.iloc[idx]['image']\n",
    "        image_tensor = self.encodedImgs[image_name]\n",
    "        image_tensor = image_tensor.permute(0,2,3,1)\n",
    "        image_tensor_view = image_tensor.view(image_tensor.size(0), -1, image_tensor.size(3))\n",
    "\n",
    "        return torch.tensor(caption_seq), torch.tensor(target_seq), image_tensor_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d8b74ef-20e2-419d-931a-69e8ef276843",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_resnet = FlickerDataSetResnet(train, 'EncodedImageTrainResNet.pkl')\n",
    "train_dataloader_resnet = DataLoader(train_dataset_resnet, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a4b78d8-8227-44b4-b490-c41fb2d8086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset_resnet = FlickerDataSetResnet(valid, 'EncodedImageValidResNet.pkl')\n",
    "valid_dataloader_resnet = DataLoader(valid_dataset_resnet, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "165f886e-cb05-4cf6-b314-ece42333af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_resnet = FlickerDataSetResnet(unq_test_imgs2, 'EncodedImageTestResNet.pkl')\n",
    "test_dataloader_resnet = DataLoader(test_dataset_resnet, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95c9cf58-777b-42d0-8a0f-77f27f4f5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pe.size(0) < x.size(0):\n",
    "            self.pe = self.pe.repeat(x.size(0), 1, 1).to(device)\n",
    "        self.pe = self.pe[:x.size(0), : , : ]\n",
    "        \n",
    "        x = x + self.pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adb4540f-ce24-487b-a639-22a5ae138e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, n_head, n_decoder_layer, vocab_size, embedding_size):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, 0.1)\n",
    "        self.TransformerDecoderLayer = nn.TransformerDecoderLayer(d_model =  embedding_size, nhead = n_head)\n",
    "        self.TransformerDecoder = nn.TransformerDecoder(decoder_layer = self.TransformerDecoderLayer, num_layers = n_decoder_layer)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_size)\n",
    "        self.last_linear_layer = nn.Linear(embedding_size, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.last_linear_layer.bias.data.zero_()\n",
    "        self.last_linear_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def generate_Mask(self, size, decoder_inp):\n",
    "        decoder_input_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        decoder_input_mask = decoder_input_mask.float().masked_fill(decoder_input_mask == 0, float('-inf')).masked_fill(decoder_input_mask == 1, float(0.0))\n",
    "\n",
    "        decoder_input_pad_mask = decoder_inp.float().masked_fill(decoder_inp == 0, float(0.0)).masked_fill(decoder_inp > 0, float(1.0))\n",
    "        decoder_input_pad_mask_bool = decoder_inp == 0\n",
    "\n",
    "        return decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool\n",
    "\n",
    "    def forward(self, encoded_image, decoder_inp):\n",
    "        encoded_image = encoded_image.permute(1,0,2)\n",
    "        \n",
    "\n",
    "        decoder_inp_embed = self.embedding(decoder_inp)* math.sqrt(self.embedding_size)\n",
    "        \n",
    "        decoder_inp_embed = self.pos_encoder(decoder_inp_embed)\n",
    "        decoder_inp_embed = decoder_inp_embed.permute(1,0,2)\n",
    "        \n",
    "\n",
    "        decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool = self.generate_Mask(decoder_inp.size(1), decoder_inp)\n",
    "        decoder_input_mask = decoder_input_mask.to(device)\n",
    "        decoder_input_pad_mask = decoder_input_pad_mask.to(device)\n",
    "        decoder_input_pad_mask_bool = decoder_input_pad_mask_bool.to(device)\n",
    "        \n",
    "\n",
    "        decoder_output = self.TransformerDecoder(tgt = decoder_inp_embed, memory = encoded_image, tgt_mask = decoder_input_mask, tgt_key_padding_mask = decoder_input_pad_mask_bool)\n",
    "        \n",
    "        final_output = self.last_linear_layer(decoder_output)\n",
    "\n",
    "        return final_output,  decoder_input_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "785a6ba6-1a5a-4567-9450-83fdddb9e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasir\\AppData\\Local\\Temp\\ipykernel_18604\\36968760.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  min_val_loss = np.float('Inf')\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 26\n",
    "ictModel = ImageCaptionModel(16, 4, vocab_size, 512).to(device)\n",
    "optimizer = torch.optim.Adam(ictModel.parameters(), lr = 0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience=2, verbose = True)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "min_val_loss = np.float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1800b-dca5-48ef-a415-ed2749eb70b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a17804d47a34e27aed2320d446f38c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch ->  0  Training Loss ->  5.348630905151367 Eval Loss ->  4.409230709075928\n",
      "Writing Model at epoch  0\n",
      "Epoch ->  1  Training Loss ->  4.239880561828613 Eval Loss ->  4.009346961975098\n",
      "Writing Model at epoch  1\n",
      "Epoch ->  2  Training Loss ->  3.9238972663879395 Eval Loss ->  3.813622236251831\n",
      "Writing Model at epoch  2\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(EPOCH)):\n",
    "    total_epoch_train_loss = 0\n",
    "    total_epoch_valid_loss = 0\n",
    "    total_train_words = 0\n",
    "    total_valid_words = 0\n",
    "    ictModel.train()\n",
    "\n",
    "    ### Train Loop\n",
    "    for caption_seq, target_seq, image_embed in train_dataloader_resnet:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_embed = image_embed.squeeze(1).to(device)\n",
    "        caption_seq = caption_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        output, padding_mask = ictModel.forward(image_embed, caption_seq)\n",
    "        output = output.permute(1, 2, 0)\n",
    "\n",
    "        loss = criterion(output,target_seq)\n",
    "\n",
    "        loss_masked = torch.mul(loss, padding_mask)\n",
    "\n",
    "        final_batch_loss = torch.sum(loss_masked)/torch.sum(padding_mask)\n",
    "\n",
    "        final_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_epoch_train_loss += torch.sum(loss_masked).detach().item()\n",
    "        total_train_words += torch.sum(padding_mask)\n",
    "\n",
    " \n",
    "    total_epoch_train_loss = total_epoch_train_loss/total_train_words\n",
    "  \n",
    "\n",
    "    ### Eval Loop\n",
    "    ictModel.eval()\n",
    "    with torch.no_grad():\n",
    "        for caption_seq, target_seq, image_embed in valid_dataloader_resnet:\n",
    "\n",
    "            image_embed = image_embed.squeeze(1).to(device)\n",
    "            caption_seq = caption_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output, padding_mask = ictModel.forward(image_embed, caption_seq)\n",
    "            output = output.permute(1, 2, 0)\n",
    "\n",
    "            loss = criterion(output,target_seq)\n",
    "\n",
    "            loss_masked = torch.mul(loss, padding_mask)\n",
    "\n",
    "            total_epoch_valid_loss += torch.sum(loss_masked).detach().item()\n",
    "            total_valid_words += torch.sum(padding_mask)\n",
    "\n",
    "    total_epoch_valid_loss = total_epoch_valid_loss/total_valid_words\n",
    "  \n",
    "    print(\"Epoch -> \", epoch,\" Training Loss -> \", total_epoch_train_loss.item(), \"Eval Loss -> \", total_epoch_valid_loss.item() )\n",
    "  \n",
    "    if min_val_loss > total_epoch_valid_loss:\n",
    "        print(\"Writing Model at epoch \", epoch)\n",
    "        torch.save(ictModel, './BestModel')\n",
    "        min_val_loss = total_epoch_valid_loss\n",
    "  \n",
    "\n",
    "    scheduler.step(total_epoch_valid_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6481b5f-a41a-4d0d-8b49-92178b45221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./BestModel')\n",
    "start_token = word_to_index['<start>']\n",
    "end_token = word_to_index['<end>']\n",
    "pad_token = word_to_index['<pad>']\n",
    "max_seq_len = 33\n",
    "print(start_token, end_token, pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707ca55-75ba-41e0-9e6c-d96ccff59569",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_img_embed = pd.read_pickle('EncodedImageValidResNet.pkl')\n",
    "test_img_embed = pd.read_pickle('EncodedImageTestResNet.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938574ac-54a6-4187-a988-cd38ce979d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(K, img_nm):\n",
    "    img_loc = '../input/flickr8k/Images/'+str(img_nm)\n",
    "    image = Image.open(img_loc).convert(\"RGB\")\n",
    "    plt.imshow(image)\n",
    "\n",
    "    model.eval() \n",
    "    valid_img_df = valid[valid['image']==img_nm]\n",
    "    print(\"Actual Caption : \")\n",
    "    print(valid_img_df['caption'].tolist())\n",
    "    \n",
    "    img_embed = valid_img_embed[img_nm].to(device)\n",
    "\n",
    "    img_embed = img_embed.permute(0,2,3,1)\n",
    "    img_embed = img_embed.view(img_embed.size(0), -1, img_embed.size(3))\n",
    "\n",
    "    input_seq = [pad_token]*max_seq_len\n",
    "    input_seq[0] = start_token\n",
    "\n",
    "    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n",
    "    predicted_sentence = []\n",
    "    with torch.no_grad():\n",
    "        for eval_iter in range(0, max_seq_len):\n",
    "\n",
    "            output, padding_mask = model.forward(img_embed, input_seq)\n",
    "\n",
    "            output = output[eval_iter, 0, :]\n",
    "\n",
    "            values = torch.topk(output, K).values.tolist()\n",
    "            indices = torch.topk(output, K).indices.tolist()\n",
    "\n",
    "            next_word_index = random.choices(indices, values, k = 1)[0]\n",
    "\n",
    "            next_word = index_to_word[next_word_index]\n",
    "\n",
    "            input_seq[:, eval_iter+1] = next_word_index\n",
    "\n",
    "\n",
    "            if next_word == '<end>' :\n",
    "                break\n",
    "\n",
    "            predicted_sentence.append(next_word)\n",
    "    print(\"\\n\")\n",
    "    print(\"Predicted caption : \")\n",
    "    print(\" \".join(predicted_sentence+['.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d9a97-7379-489b-b6a4-cd055a58df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_test(K, img_nm):\n",
    "    img_loc = '../input/multimodal-hate-speech/img_resized/'+str(img_nm)\n",
    "    image = Image.open(img_loc).convert(\"RGB\")\n",
    "    plt.imshow(image)\n",
    "\n",
    "    model.eval() \n",
    "    \n",
    "    img_embed = test_img_embed[img_nm].to(device)\n",
    "\n",
    "    img_embed = img_embed.permute(0,2,3,1)\n",
    "    img_embed = img_embed.view(img_embed.size(0), -1, img_embed.size(3))\n",
    "\n",
    "    input_seq = [pad_token]*max_seq_len\n",
    "    input_seq[0] = start_token\n",
    "\n",
    "    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n",
    "    predicted_sentence = []\n",
    "    with torch.no_grad():\n",
    "        for eval_iter in range(0, max_seq_len):\n",
    "\n",
    "            output, padding_mask = model.forward(img_embed, input_seq)\n",
    "\n",
    "            output = output[eval_iter, 0, :]\n",
    "\n",
    "            values = torch.topk(output, K).values.tolist()\n",
    "            indices = torch.topk(output, K).indices.tolist()\n",
    "\n",
    "            next_word_index = random.choices(indices, values, k = 1)[0]\n",
    "\n",
    "            next_word = index_to_word[next_word_index]\n",
    "\n",
    "            input_seq[:, eval_iter+1] = next_word_index\n",
    "\n",
    "\n",
    "            if next_word == '<end>' :\n",
    "                break\n",
    "\n",
    "            predicted_sentence.append(next_word)\n",
    "    print(\"\\n\")\n",
    "    print(\"Predicted caption : \")\n",
    "    print(\" \".join(predicted_sentence+['.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e127922-b71e-47a4-8c36-ae708c236704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption(K, img_nm):\n",
    "    img_embed = test_img_embed[img_nm].to(device)\n",
    "    img_embed = img_embed.permute(0,2,3,1)\n",
    "    img_embed = img_embed.view(img_embed.size(0), -1, img_embed.size(3))\n",
    "    input_seq = [pad_token]*max_seq_len\n",
    "    input_seq[0] = start_token\n",
    "    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n",
    "    predicted_sentence = []\n",
    "    with torch.no_grad():\n",
    "        for eval_iter in range(0, max_seq_len):\n",
    "\n",
    "            output, padding_mask = model.forward(img_embed, input_seq)\n",
    "\n",
    "            output = output[eval_iter, 0, :]\n",
    "\n",
    "            values = torch.topk(output, K).values.tolist()\n",
    "            indices = torch.topk(output, K).indices.tolist()\n",
    "\n",
    "            next_word_index = random.choices(indices, values, k = 1)[0]\n",
    "\n",
    "            next_word = index_to_word[next_word_index]\n",
    "\n",
    "            input_seq[:, eval_iter+1] = next_word_index\n",
    "\n",
    "\n",
    "            if next_word == '<end>' :\n",
    "                break\n",
    "\n",
    "            predicted_sentence.append(next_word)\n",
    "#     print(\"\\n\")\n",
    "#     print(\"Predicted caption : \")\n",
    "    output = \" \".join(predicted_sentence+['.'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a566743-4198-4853-abaa-8dadf8df07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(2, unq_valid_imgs.iloc[71]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031da7a8-3aa5-4789-a727-62d4408cbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "\n",
    "results = pd.DataFrame({'image': [], 'gen_caption': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0250a63-1a80-4be1-b7b8-b62555b178f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(unq_test_imgs2)):\n",
    "    try:\n",
    "        if not i%50: print(i)\n",
    "        results = pd.concat([results, pd.DataFrame({'id' : [unq_test_imgs.iloc[i]['id']], 'image': [unq_test_imgs.iloc[i]['image']], \n",
    "                                                'gen_caption': [get_caption(1, unq_test_imgs.iloc[i]['image'])]})], \n",
    "                            ignore_index=True)\n",
    "    except:\n",
    "        print(\"Error occured at: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295658d3-65ac-44f6-ab6b-dbd4cebd7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_ress = results.drop_duplicates()\n",
    "unq_ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841249d2-07e1-44d0-94da-11daa2f79908",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_ress.to_csv(\"./dev_image_captions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef5389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f7660018a1f48e52e1285a6cd46b08bf9e759a1146999e72bfa769101423a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
