{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e09059-1050-45c2-89cb-ac6581497f80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from getpass import getpass\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785d9e59-421b-437b-9431-d02236ce610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.layers import nms\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2d7394-2185-4034-acbd-15fb092cba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bordered_resize(img, scale, center=None, border_size=None, border_color=None):\n",
    "    if center == None:\n",
    "        center = (int(img.shape[1]), int(img.shape[0]))\n",
    "    if border_size == None:\n",
    "        border_size = (img.shape[1], img.shape[0])\n",
    "    if border_color == None:\n",
    "        border_color=(255, 255, 255)\n",
    "    M = cv2.getRotationMatrix2D(center, 0, scale)\n",
    "    return cv2.warpAffine(img, M, border_size, borderValue=border_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abdffe73-2253-44bc-b646-e78c22c12977",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "\n",
    "def load_config_and_model_weights(cfg_path):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
    "\n",
    "    # ROI HEADS SCORE THRESHOLD\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "    # Comment the next line if you're using 'cuda'\n",
    "    cfg['MODEL']['DEVICE']='cpu'\n",
    "\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "cfg = load_config_and_model_weights(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3667596b-c410-42c6-8199-d669e2b3800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_final_a3ec72.pkl: 254MB [03:48, 1.11MB/s]                                                                        \n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def get_model(cfg):\n",
    "    # build model\n",
    "    model = build_model(cfg)\n",
    "\n",
    "    # load weights\n",
    "    checkpointer = DetectionCheckpointer(model)\n",
    "    checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "    # eval mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = get_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc959b35-68be-44a2-af8a-e16000dfe4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_inputs(cfg, img_list):\n",
    "    # Resizing the image according to the configuration\n",
    "    transform_gen = T.ResizeShortestEdge(\n",
    "                [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "            )\n",
    "    img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
    "\n",
    "    # Convert to C,H,W format\n",
    "    convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "    batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
    "\n",
    "    # Normalizing the image\n",
    "    num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "    pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
    "    pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
    "    normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
    "    images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
    "\n",
    "    # Convert to ImageList\n",
    "    images =  ImageList.from_tensors(images,model.backbone.size_divisibility)\n",
    "    \n",
    "    return images, batched_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf9a16f-fbd0-42cc-8d1f-fc218b3a97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, images):\n",
    "    features = model.backbone(images.tensor)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ad1f45-cd46-4b09-bc22-1f3f426524fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proposals(model, images, features):\n",
    "    proposals, _ = model.proposal_generator(images, features)\n",
    "    return proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8df631-bc4e-46ef-af0c-3b58f8871c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_features(model, features, proposals):\n",
    "    features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
    "    box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    box_features = model.roi_heads.box_head.flatten(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc_relu1(box_features)\n",
    "    box_features = model.roi_heads.box_head.fc2(box_features)\n",
    "\n",
    "    box_features = box_features.reshape(1, 1000, 1024) # depends on your config and batch size\n",
    "    return box_features, features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c5484c9-8d41-40c4-befd-92c7d6834532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_logits(model, features_list, proposals):\n",
    "    cls_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "    cls_features = model.roi_heads.box_head(cls_features)\n",
    "    pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(cls_features)\n",
    "    return pred_class_logits, pred_proposal_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0510ed7-5779-4fe0-9424-0889d7923dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_scores(cfg, pred_class_logits, pred_proposal_deltas):\n",
    "    box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "    smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "\n",
    "    outputs = FastRCNNOutputs(\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        smooth_l1_beta,\n",
    "    )\n",
    "\n",
    "    boxes = outputs.predict_boxes()\n",
    "    scores = outputs.predict_probs()\n",
    "    image_shapes = outputs.image_shapes\n",
    "\n",
    "    return boxes, scores, image_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "744ffa5a-e7d5-4155-b18b-d7050127991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_boxes(boxes, batched_inputs, image_size):\n",
    "    proposal_boxes = boxes.reshape(-1, 4).clone()\n",
    "    scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
    "    output_boxes = Boxes(proposal_boxes)\n",
    "\n",
    "    output_boxes.scale(scale_x, scale_y)\n",
    "    output_boxes.clip(image_size)\n",
    "\n",
    "    return output_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace49a01-2f17-47a7-846b-93ac221e9f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_boxes(cfg, output_boxes, scores):\n",
    "    test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "    test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "    cls_prob = scores.detach()\n",
    "    cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
    "    max_conf = torch.zeros((cls_boxes.shape[0]))\n",
    "    for cls_ind in range(0, cls_prob.shape[1]-1):\n",
    "        cls_scores = cls_prob[:, cls_ind+1]\n",
    "        det_boxes = cls_boxes[:,cls_ind,:]\n",
    "        keep = np.array(nms(det_boxes, cls_scores, test_nms_thresh))\n",
    "        max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
    "    keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
    "    return keep_boxes, max_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db495c8-9742-4f5c-ab4f-0a559c8321c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BOXES=10\n",
    "MAX_BOXES=100\n",
    "def filter_boxes(keep_boxes, max_conf, min_boxes, max_boxes):\n",
    "    if len(keep_boxes) < min_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
    "    elif len(keep_boxes) > max_boxes:\n",
    "        keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
    "    return keep_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2acd30d3-535f-431a-908b-d26c9c884c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visual_embeds(box_features, keep_boxes):\n",
    "    return box_features[keep_boxes.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b310c7b-d4d7-4f26-96ec-e1f11a952b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for file_name in os.listdir(\"Data/img  \"):\n",
    "  file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa84bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "import os\n",
    "for file_name in file_names:\n",
    "  img_list = []\n",
    "  name = \"img_\" + file_name\n",
    "  name = plt.imread(\"Data/img/\"+file_name)\n",
    "  name = cv2.resize(name, (365, 252)) \n",
    "  namef = \"img_\" + file_name + \"_bgr\"\n",
    "  namef = cv2.cvtColor(name, cv2.COLOR_RGB2BGR)\n",
    "  img_list.append(namef)\n",
    "  images, batched_inputs = prepare_image_inputs(cfg, img_list)\n",
    "  features = get_features(model, images)\n",
    "  proposals = get_proposals(model, images, features)\n",
    "  box_features, features_list = get_box_features(model, features, proposals)\n",
    "  pred_class_logits, pred_proposal_deltas = get_prediction_logits(model, features_list, proposals)\n",
    "  boxes, scores, image_shapes = get_box_scores(cfg, pred_class_logits, pred_proposal_deltas)\n",
    "  output_boxes = [get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]\n",
    "  temp = [select_boxes(cfg, output_boxes[i], scores[i]) for i in range(len(scores))]\n",
    "  keep_boxes, max_conf = [],[]\n",
    "  for keep_box, mx_conf in temp:\n",
    "    keep_boxes.append(keep_box)\n",
    "    max_conf.append(mx_conf)\n",
    "  MIN_BOXES=10\n",
    "  MAX_BOXES=100\n",
    "  keep_boxes = [filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]\n",
    "  visual_embeds = [get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
    "  np.save(\"Emb_feature/test_features/\"+file_name[:-4],visual_embeds[0].detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f7660018a1f48e52e1285a6cd46b08bf9e759a1146999e72bfa769101423a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
