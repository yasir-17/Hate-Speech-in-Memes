{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preserve_unused_tokens=False']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from absl import flags\n",
    "sys.argv=['preserve_unused_tokens=False']\n",
    "flags.FLAGS(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in c:\\users\\yasir\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\yasir\\anaconda3\\lib\\site-packages (from bert-tensorflow) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading tokenization script created by the Google\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, Dropout, Input, LSTM, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8313, 100, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_feature = np.load('Emb_feature/new_test_features/.npy')\n",
    "np.shape(img_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the picture man in black and white cap is holding up sign . And the text says: its their character not their color that matters \n",
      "\n",
      "\n",
      "8313\n"
     ]
    }
   ],
   "source": [
    "FMS = pd.read_csv(\"FMS_final.csv\")\n",
    "FMS['textNdesc'] = 'In the picture '+ FMS.gen_caption + ' And the text says: ' + FMS.text\n",
    "print(FMS.textNdesc[0], '\\n\\n')\n",
    "print(len(FMS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>image</th>\n",
       "      <th>gen_caption</th>\n",
       "      <th>textNdesc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "      <td>42953.png</td>\n",
       "      <td>man in black and white cap is holding up sign .</td>\n",
       "      <td>In the picture man in black and white cap is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "      <td>23058.png</td>\n",
       "      <td>man wearing black shirt and white shirt is sta...</td>\n",
       "      <td>In the picture man wearing black shirt and whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "      <td>13894.png</td>\n",
       "      <td>small child is jumping over the snow .</td>\n",
       "      <td>In the picture small child is jumping over the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "      <td>37408.png</td>\n",
       "      <td>black dog with blue collar is walking through ...</td>\n",
       "      <td>In the picture black dog with blue collar is w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "      <td>82403.png</td>\n",
       "      <td>two men are standing in front of white building .</td>\n",
       "      <td>In the picture two men are standing in front o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>10423</td>\n",
       "      <td>img/10423.png</td>\n",
       "      <td>1</td>\n",
       "      <td>nobody wants to hang auschwitz me</td>\n",
       "      <td>10423.png</td>\n",
       "      <td>two men are sitting on the ground and one is w...</td>\n",
       "      <td>In the picture two men are sitting on the grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8309</th>\n",
       "      <td>98203</td>\n",
       "      <td>img/98203.png</td>\n",
       "      <td>1</td>\n",
       "      <td>when god grants you a child after 20 years of ...</td>\n",
       "      <td>98203.png</td>\n",
       "      <td>man with sunglasses and sunglasses is holding ...</td>\n",
       "      <td>In the picture man with sunglasses and sunglas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8310</th>\n",
       "      <td>36947</td>\n",
       "      <td>img/36947.png</td>\n",
       "      <td>1</td>\n",
       "      <td>gays on social media: equality! body positivit...</td>\n",
       "      <td>36947.png</td>\n",
       "      <td>woman with long hair and black hair is smiling .</td>\n",
       "      <td>In the picture woman with long hair and black ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8311</th>\n",
       "      <td>16492</td>\n",
       "      <td>img/16492.png</td>\n",
       "      <td>1</td>\n",
       "      <td>having a bad day? you could be a siamese twin ...</td>\n",
       "      <td>16492.png</td>\n",
       "      <td>two men wearing black and white shirts and whi...</td>\n",
       "      <td>In the picture two men wearing black and white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8312</th>\n",
       "      <td>15937</td>\n",
       "      <td>img/15937.png</td>\n",
       "      <td>1</td>\n",
       "      <td>i hate muslims too they take their religion to...</td>\n",
       "      <td>15937.png</td>\n",
       "      <td>small child is playing with white and blue bla...</td>\n",
       "      <td>In the picture small child is playing with whi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8313 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            img  label  \\\n",
       "0     42953  img/42953.png      0   \n",
       "1     23058  img/23058.png      0   \n",
       "2     13894  img/13894.png      0   \n",
       "3     37408  img/37408.png      0   \n",
       "4     82403  img/82403.png      0   \n",
       "...     ...            ...    ...   \n",
       "8308  10423  img/10423.png      1   \n",
       "8309  98203  img/98203.png      1   \n",
       "8310  36947  img/36947.png      1   \n",
       "8311  16492  img/16492.png      1   \n",
       "8312  15937  img/15937.png      1   \n",
       "\n",
       "                                                   text      image  \\\n",
       "0      its their character not their color that matters  42953.png   \n",
       "1     don't be afraid to love again everyone is not ...  23058.png   \n",
       "2                              putting bows on your pet  13894.png   \n",
       "3     i love everything and everybody! except for sq...  37408.png   \n",
       "4     everybody loves chocolate chip cookies, even h...  82403.png   \n",
       "...                                                 ...        ...   \n",
       "8308                  nobody wants to hang auschwitz me  10423.png   \n",
       "8309  when god grants you a child after 20 years of ...  98203.png   \n",
       "8310  gays on social media: equality! body positivit...  36947.png   \n",
       "8311  having a bad day? you could be a siamese twin ...  16492.png   \n",
       "8312  i hate muslims too they take their religion to...  15937.png   \n",
       "\n",
       "                                            gen_caption  \\\n",
       "0       man in black and white cap is holding up sign .   \n",
       "1     man wearing black shirt and white shirt is sta...   \n",
       "2                small child is jumping over the snow .   \n",
       "3     black dog with blue collar is walking through ...   \n",
       "4     two men are standing in front of white building .   \n",
       "...                                                 ...   \n",
       "8308  two men are sitting on the ground and one is w...   \n",
       "8309  man with sunglasses and sunglasses is holding ...   \n",
       "8310   woman with long hair and black hair is smiling .   \n",
       "8311  two men wearing black and white shirts and whi...   \n",
       "8312  small child is playing with white and blue bla...   \n",
       "\n",
       "                                              textNdesc  \n",
       "0     In the picture man in black and white cap is h...  \n",
       "1     In the picture man wearing black shirt and whi...  \n",
       "2     In the picture small child is jumping over the...  \n",
       "3     In the picture black dog with blue collar is w...  \n",
       "4     In the picture two men are standing in front o...  \n",
       "...                                                 ...  \n",
       "8308  In the picture two men are sitting on the grou...  \n",
       "8309  In the picture man with sunglasses and sunglas...  \n",
       "8310  In the picture woman with long hair and black ...  \n",
       "8311  In the picture two men wearing black and white...  \n",
       "8312  In the picture small child is playing with whi...  \n",
       "\n",
       "[8313 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(bert_layer, max_len=512):\n",
    "#     input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "#     input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "#     segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "#     image_input = tf.keras.Input(shape=(100,1024), batch_size=None, name=\"image_input\")\n",
    "#     image_flatten = tf.keras.layers.Flatten()(image_input)\n",
    "#     image_dense_1 = tf.keras.layers.Dense(4096, activation = tf.nn.relu, kernel_initializer = tf.keras.initializers.he_uniform(seed=54))(image_flatten)\n",
    "#     image_dense_2 = tf.keras.layers.Dense(1024, activation = tf.nn.relu, kernel_initializer = tf.keras.initializers.he_uniform(seed=32))(image_dense_1)\n",
    "\n",
    "\n",
    "#     _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "#     clf_output = sequence_output[:, 0, :]\n",
    "\n",
    "#     print(np.shape(sequence_output))\n",
    "#     print(np.shape(clf_output))\n",
    "\n",
    "#     image_question = tf.keras.layers.Multiply()([image_dense_2, clf_output])\n",
    "#     image_question_dense_1 = tf.keras.layers.Dense(1000, activation = tf.nn.relu, kernel_initializer = tf.keras.initializers.he_uniform(seed=19))(image_question)\n",
    "#     image_question_dense_2 = tf.keras.layers.Dense(1000, activation = tf.nn.relu, kernel_initializer = tf.keras.initializers.he_uniform(seed=28))(image_question_dense_1)\n",
    "#     output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, kernel_initializer = tf.keras.initializers.glorot_normal(seed=15))(image_question_dense_2)\n",
    "    \n",
    "#     # out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "#     model = Model(inputs=[input_word_ids, input_mask, segment_ids, image_input], outputs=output)\n",
    "#     model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMaps(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  Given an image feature map V ∈ R(d×N), and the question representation Q ∈ R(d×T), \n",
    "  calculates the affinity matrix C ∈ R(T×N): C = tanh((QT)(Wb)V) ; \n",
    "  where Wb ∈ R(d×d) contains the weights. (Refer eqt (3) section 3.3).\n",
    "  Given this affinity matrix C ∈ R(T×N), predicts image and question attention maps \n",
    "  (Refer eqt (4) section 3.3).\n",
    "  Arguments:\n",
    "    dim_k     : Hidden attention dimention\n",
    "    reg_value : Regularization value\n",
    "  Inputs:\n",
    "    image_feat,    V : shape (N,  d) or (49, dim_d)\n",
    "    ques_feat,     Q : shape (T,  d) or (23, dim_d)\n",
    "  Outputs:\n",
    "    Image and Question attention maps viz:\n",
    "    a) Hv = tanh(WvV + (WqQ)C) and\n",
    "    b) Hq = tanh(WqQ + (WvV )CT)\n",
    "  \"\"\"\n",
    "  def __init__(self, dim_k, reg_value, **kwargs):\n",
    "    super(AttentionMaps, self).__init__(**kwargs)\n",
    "\n",
    "    self.dim_k = dim_k\n",
    "    self.reg_value = reg_value\n",
    "\n",
    "    self.Wv = Dense(self.dim_k, activation=None,\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=2))\n",
    "    self.Wq = Dense(self.dim_k, activation=None,\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=3))\n",
    "\n",
    "  def call(self, image_feat, ques_feat):\n",
    "    \"\"\"\n",
    "    The main logic of this layer.\n",
    "    \"\"\"  \n",
    "\n",
    "    # Affinity Matrix C\n",
    "    # (QT)(Wb)V \n",
    "    C = tf.matmul(ques_feat, tf.transpose(image_feat, perm=[0,2,1])) # [b, 23, 49]\n",
    "    # tanh((QT)(Wb)V)\n",
    "    C = tf.keras.activations.tanh(C) \n",
    "\n",
    "    # (Wv)V\n",
    "    WvV = self.Wv(image_feat)                             # [b, 49, dim_k]\n",
    "    # (Wq)Q\n",
    "    WqQ = self.Wq(ques_feat)                              # [b, 23, dim_k]\n",
    "\n",
    "    # ((Wq)Q)C\n",
    "    WqQ_C = tf.matmul(tf.transpose(WqQ, perm=[0,2,1]), C) # [b, k, 49]\n",
    "    WqQ_C = tf.transpose(WqQ_C, perm =[0,2,1])            # [b, 49, k]\n",
    "\n",
    "    # ((Wv)V)CT                                           # [b, k, 23]\n",
    "    WvV_C = tf.matmul(tf.transpose(WvV, perm=[0,2,1]), tf.transpose(C, perm=[0,2,1]))  \n",
    "                        \n",
    "    WvV_C = tf.transpose(WvV_C, perm =[0,2,1])            # [b, 23, k]\n",
    "\n",
    "    #---------------image attention map------------------\n",
    "    # We find \"Hv = tanh((Wv)V + ((Wq)Q)C)\" ; H_v shape [49, k]\n",
    "\n",
    "    H_v = WvV + WqQ_C                                     # (Wv)V + ((Wq)Q)C\n",
    "    H_v = tf.keras.activations.tanh(H_v)                  # tanh((Wv)V + ((Wq)Q)C) \n",
    "\n",
    "    #---------------question attention map---------------\n",
    "    # We find \"Hq = tanh((Wq)Q + ((Wv)V)CT)\" ; H_q shape [23, k]\n",
    "\n",
    "    H_q = WqQ + WvV_C                                     # (Wq)Q + ((Wv)V)CT\n",
    "    H_q = tf.keras.activations.tanh(H_q)                  # tanh((Wq)Q + ((Wv)V)CT) \n",
    "        \n",
    "    return [H_v, H_q]                                     # [b, 49, k], [b, 23, k]\n",
    "  \n",
    "  def get_config(self):\n",
    "    \"\"\"\n",
    "    This method collects the input shape and other information about the layer.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'dim_k': self.dim_k,\n",
    "        'reg_value': self.reg_value\n",
    "    }\n",
    "    base_config = super(AttentionMaps, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextVector(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  Method to find context vector of the image and text features\n",
    "  (Refer eqt (4) and (5) section 3.3).\n",
    "  \n",
    "  Arguments:\n",
    "    reg_value : Regularization value\n",
    "    \n",
    "  Inputs:\n",
    "    image_feat V: image features, (49, d)\n",
    "    ques_feat  Q: question features, (23, d)\n",
    "    H_v: image attention map, (49, k)\n",
    "    H_q: question attention map, (23, k)\n",
    "  Outputs:\n",
    "    Returns d-dimenstional context vector for image and question features\n",
    "  \"\"\"\n",
    "  def __init__(self, reg_value, **kwargs):\n",
    "    super(ContextVector, self).__init__(**kwargs)\n",
    "\n",
    "    self.reg_value = reg_value\n",
    "\n",
    "    self.w_hv = Dense(1, activation='softmax',\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=4))\n",
    "    self.w_hq = Dense(1, activation='softmax',\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=5)) \n",
    "    \n",
    "\n",
    "  def call(self, image_feat, ques_feat, H_v, H_q):\n",
    "    \"\"\"\n",
    "    The main logic of this layer.\n",
    "    \"\"\"  \n",
    "    # attention probabilities of each image region vn; a_v = softmax(wT_hv * H_v)\n",
    "    a_v = self.w_hv(H_v)                               # [b, 49, 1]\n",
    "\n",
    "    # attention probabilities of each word qt ;        a_q = softmax(wT_hq * H_q)\n",
    "    a_q = self.w_hq(H_q)                               # [b, 23, 1]\n",
    "\n",
    "    # context vector for image\n",
    "    v = a_v * image_feat                               # [b, 49, dim_d]\n",
    "    v = tf.reduce_sum(v, 1)                            # [b, dim_d]\n",
    "\n",
    "    # context vector for question\n",
    "    q = a_q * ques_feat                                # [b, 23, dim_d]\n",
    "    q = tf.reduce_sum(q, 1)                            # [b, dim_d]\n",
    "\n",
    "\n",
    "    return [v, q]\n",
    "\n",
    "  def get_config(self):\n",
    "    \"\"\"\n",
    "    This method collects the input shape and other information about the layer.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'reg_value': self.reg_value\n",
    "    }\n",
    "    base_config = super(ContextVector, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseLevelFeatures(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  We compute the phrase features by applying 1-D convolution on the word embedding \n",
    "  vectors with filters of three window sizes: unigram, bigram and trigram.\n",
    "  The word-level features Qw are appropriately 0-padded before feeding into bigram and \n",
    "  trigram convolutions to maintain the length of the sequence after convolution.\n",
    "  Given the convolution result, we then apply max-pooling across different n-grams at each word\n",
    "  location to obtain phrase-level features\n",
    "  (Refer eqt (1) and (2) section 3.2).\n",
    "  Arguments:\n",
    "    dim_d: hidden dimension\n",
    "  Inputs:\n",
    "    word_feat Q : word level features of shape (23, dim_d)\n",
    "  Outputs:\n",
    "    Phrase level features of the question of shape (23, dim_d)\n",
    "  \"\"\"\n",
    "  def __init__(self, dim_d, **kwargs):\n",
    "    super(PhraseLevelFeatures, self).__init__(**kwargs)\n",
    "    \n",
    "    self.dim_d = dim_d\n",
    "    \n",
    "    self.conv_unigram = Conv1D(self.dim_d, kernel_size=1, strides=1,\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=6)) \n",
    "    self.conv_bigram =  Conv1D(self.dim_d, kernel_size=2, strides=1, padding='same',\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=7)) \n",
    "    self.conv_trigram = Conv1D(self.dim_d, kernel_size=3, strides=1, padding='same',\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=8)) \n",
    "\n",
    "\n",
    "  def call(self, word_feat):\n",
    "    \"\"\"\n",
    "    The main logic of this layer.\n",
    "    Compute the n-gram phrase embeddings (n=1,2,3)\n",
    "    \"\"\"\n",
    "    # phrase level unigram features\n",
    "    x_uni = self.conv_unigram(word_feat)                    \n",
    "\n",
    "    # phrase level bigram features\n",
    "    x_bi  = self.conv_bigram(word_feat)                     \n",
    "\n",
    "    # phrase level trigram features\n",
    "    x_tri = self.conv_trigram(word_feat)                    \n",
    "\n",
    "    # Concat\n",
    "    x = tf.concat([tf.expand_dims(x_uni, -1),\\\n",
    "                    tf.expand_dims(x_bi, -1),\\\n",
    "                    tf.expand_dims(x_tri, -1)], -1)         \n",
    "\n",
    "    # https://stackoverflow.com/a/36853403\n",
    "    # Max-pool across n-gram features; over-all phrase level feature\n",
    "    x = tf.reduce_max(x, -1)                                \n",
    "\n",
    "    return x\n",
    "\n",
    "  def get_config(self):\n",
    "    \"\"\"\n",
    "    This method collects the input shape and other information about the layer.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'dim_d': self.dim_d\n",
    "    }\n",
    "    base_config = super(PhraseLevelFeatures, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len, dim_d, dim_k, l_rate, d_rate, reg_value):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    print(np.shape(sequence_output))\n",
    "    print(np.shape(clf_output))\n",
    "\n",
    "    image_input = tf.keras.Input(shape=(100,1024), batch_size=None, name=\"image_input\")\n",
    "    # image feature; (Wb)V                                          # [b, 49, dim_d]\n",
    "    image_feat = Dense(dim_d, activation=None, name='Image_Feat_Dense', kernel_regularizer=tf.keras.regularizers.l2(reg_value), kernel_initializer=tf.keras.initializers.glorot_uniform(seed=1))(image_input)\n",
    "    image_feat = Dropout(d_rate, seed=1)(image_feat)\n",
    "\n",
    "    Hv_w, Hq_w = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Word')(image_feat, sequence_output)\n",
    "    v_w, q_w = ContextVector(reg_value, name='ContextVector_Word')(image_feat, sequence_output, Hv_w, Hq_w)\n",
    "    feat_w = tf.add(v_w,q_w)\n",
    "    h_w = Dense(dim_d, activation='tanh', name='h_w_Dense', kernel_regularizer=tf.keras.regularizers.l2(reg_value), kernel_initializer=tf.keras.initializers.glorot_uniform(seed=13))(feat_w)\n",
    "\n",
    "\n",
    "    # phrase level\n",
    "    text_feat_p = PhraseLevelFeatures(dim_d, name='PhraseLevelFeatures')(sequence_output)\n",
    "\n",
    "    Hv_p, Hq_p = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Phrase')(image_feat, text_feat_p)\n",
    "    v_p, q_p = ContextVector(reg_value, name='ContextVector_Phrase')(image_feat, text_feat_p, Hv_p, Hq_p)\n",
    "    feat_p = concatenate([tf.add(v_p,q_p), h_w], -1) \n",
    "    h_p = Dense(dim_d, activation='tanh', name='h_p_Dense', kernel_regularizer=tf.keras.regularizers.l2(reg_value), kernel_initializer=tf.keras.initializers.glorot_uniform(seed=14))(feat_p)\n",
    "\n",
    "    # sentence level\n",
    "    text_feat_s = LSTM(dim_d, return_sequences=True, input_shape=(None, max_len, dim_d),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(text_feat_p)\n",
    "\n",
    "    Hv_s, Hq_s = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Sent')(image_feat, text_feat_s)\n",
    "    v_s, q_s = ContextVector(reg_value, name='ContextVector_Sent')(image_feat, text_feat_p, Hv_s, Hq_s)\n",
    "    feat_s = concatenate([tf.add(v_s,q_s), h_p], -1) \n",
    "    h_s = Dense(2*dim_d, activation='tanh', name='h_s_Dense',\\\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=15))(feat_s)\n",
    "\n",
    "    z   = Dense(2*dim_d, activation='tanh', name='z_Dense',\\\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(h_s)\n",
    "    z   = Dropout(d_rate, seed=16)(z)\n",
    "\n",
    "    # result\n",
    "    result = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids, image_input], outputs=result)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(FMS.textNdesc.values, tokenizer, max_len=100)\n",
    "train_labels = FMS.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 1024)\n",
      "(None, 1024)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 100, 1024)]  0           []                               \n",
      "                                                                                                  \n",
      " Image_Feat_Dense (Dense)       (None, 100, 1024)    1049600     ['image_input[0][0]']            \n",
      "                                                                                                  \n",
      " input_word_ids (InputLayer)    [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 100, 1024)    0           ['Image_Feat_Dense[0][0]']       \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       [(None, 1024),       335141889   ['input_word_ids[0][0]',         \n",
      "                                 (None, 100, 1024)]               'input_mask[0][0]',             \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " PhraseLevelFeatures (PhraseLev  (None, 100, 1024)   6294528     ['keras_layer[0][1]']            \n",
      " elFeatures)                                                                                      \n",
      "                                                                                                  \n",
      " AttentionMaps_Word (AttentionM  [(None, 100, 256),  524800      ['dropout[0][0]',                \n",
      " aps)                            (None, 100, 256)]                'keras_layer[0][1]']            \n",
      "                                                                                                  \n",
      " AttentionMaps_Phrase (Attentio  [(None, 100, 256),  524800      ['dropout[0][0]',                \n",
      " nMaps)                          (None, 100, 256)]                'PhraseLevelFeatures[0][0]']    \n",
      "                                                                                                  \n",
      " ContextVector_Word (ContextVec  [(None, 1024),      514         ['dropout[0][0]',                \n",
      " tor)                            (None, 1024)]                    'keras_layer[0][1]',            \n",
      "                                                                  'AttentionMaps_Word[0][0]',     \n",
      "                                                                  'AttentionMaps_Word[0][1]']     \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 100, 1024)    8392704     ['PhraseLevelFeatures[0][0]']    \n",
      "                                                                                                  \n",
      " ContextVector_Phrase (ContextV  [(None, 1024),      514         ['dropout[0][0]',                \n",
      " ector)                          (None, 1024)]                    'PhraseLevelFeatures[0][0]',    \n",
      "                                                                  'AttentionMaps_Phrase[0][0]',   \n",
      "                                                                  'AttentionMaps_Phrase[0][1]']   \n",
      "                                                                                                  \n",
      " tf.math.add (TFOpLambda)       (None, 1024)         0           ['ContextVector_Word[0][0]',     \n",
      "                                                                  'ContextVector_Word[0][1]']     \n",
      "                                                                                                  \n",
      " AttentionMaps_Sent (AttentionM  [(None, 100, 256),  524800      ['dropout[0][0]',                \n",
      " aps)                            (None, 100, 256)]                'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.add_1 (TFOpLambda)     (None, 1024)         0           ['ContextVector_Phrase[0][0]',   \n",
      "                                                                  'ContextVector_Phrase[0][1]']   \n",
      "                                                                                                  \n",
      " h_w_Dense (Dense)              (None, 1024)         1049600     ['tf.math.add[0][0]']            \n",
      "                                                                                                  \n",
      " ContextVector_Sent (ContextVec  [(None, 1024),      514         ['dropout[0][0]',                \n",
      " tor)                            (None, 1024)]                    'PhraseLevelFeatures[0][0]',    \n",
      "                                                                  'AttentionMaps_Sent[0][0]',     \n",
      "                                                                  'AttentionMaps_Sent[0][1]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2048)         0           ['tf.math.add_1[0][0]',          \n",
      "                                                                  'h_w_Dense[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.add_2 (TFOpLambda)     (None, 1024)         0           ['ContextVector_Sent[0][0]',     \n",
      "                                                                  'ContextVector_Sent[0][1]']     \n",
      "                                                                                                  \n",
      " h_p_Dense (Dense)              (None, 1024)         2098176     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2048)         0           ['tf.math.add_2[0][0]',          \n",
      "                                                                  'h_p_Dense[0][0]']              \n",
      "                                                                                                  \n",
      " h_s_Dense (Dense)              (None, 2048)         4196352     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " z_Dense (Dense)                (None, 2048)         4196352     ['h_s_Dense[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 2048)         0           ['z_Dense[0][0]']                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            2049        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 363,997,192\n",
      "Trainable params: 28,855,303\n",
      "Non-trainable params: 335,141,889\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yasir\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len = 100, dim_d = 1024, dim_k = 256, l_rate = 1e-4, d_rate = 0.3, reg_value = 0.01)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1034s 91s/step - loss: 100.2229 - accuracy: 0.6250 - auc: 0.4977 - val_loss: 99.9012 - val_accuracy: 0.5866 - val_auc: 0.5698\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 968s 87s/step - loss: 99.6304 - accuracy: 0.6484 - auc: 0.5280 - val_loss: 99.3635 - val_accuracy: 0.6055 - val_auc: 0.5552\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 956s 86s/step - loss: 99.1213 - accuracy: 0.5807 - auc: 0.4992 - val_loss: 98.8004 - val_accuracy: 0.6055 - val_auc: 0.5776\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1006s 90s/step - loss: 98.5483 - accuracy: 0.6458 - auc: 0.4673 - val_loss: 98.2777 - val_accuracy: 0.6055 - val_auc: 0.5607\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 960s 86s/step - loss: 98.0486 - accuracy: 0.5625 - auc: 0.5044 - val_loss: 97.7012 - val_accuracy: 0.6047 - val_auc: 0.5945\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 958s 86s/step - loss: 97.4407 - accuracy: 0.6797 - auc: 0.5678 - val_loss: 97.1702 - val_accuracy: 0.6055 - val_auc: 0.5977\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 961s 86s/step - loss: 96.9584 - accuracy: 0.5625 - auc: 0.5447 - val_loss: 96.6622 - val_accuracy: 0.6055 - val_auc: 0.6063\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 956s 86s/step - loss: 96.4054 - accuracy: 0.6328 - auc: 0.4764 - val_loss: 96.1245 - val_accuracy: 0.6055 - val_auc: 0.5782\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 953s 85s/step - loss: 95.8960 - accuracy: 0.5938 - auc: 0.4760 - val_loss: 95.5821 - val_accuracy: 0.6055 - val_auc: 0.5944\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 958s 86s/step - loss: 95.3488 - accuracy: 0.6536 - auc: 0.5054 - val_loss: 95.0564 - val_accuracy: 0.6055 - val_auc: 0.6055\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 952s 85s/step - loss: 94.8063 - accuracy: 0.6484 - auc: 0.5701 - val_loss: 94.5961 - val_accuracy: 0.6055 - val_auc: 0.6017\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 949s 85s/step - loss: 94.3216 - accuracy: 0.6250 - auc: 0.5315 - val_loss: 94.0168 - val_accuracy: 0.6055 - val_auc: 0.6113\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 957s 86s/step - loss: 93.8254 - accuracy: 0.6042 - auc: 0.4866 - val_loss: 93.5015 - val_accuracy: 0.6055 - val_auc: 0.6222\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 972s 87s/step - loss: 93.2634 - accuracy: 0.6328 - auc: 0.5441 - val_loss: 93.1067 - val_accuracy: 0.6055 - val_auc: 0.6099\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 938s 84s/step - loss: 92.8117 - accuracy: 0.6224 - auc: 0.4804 - val_loss: 92.4995 - val_accuracy: 0.6203 - val_auc: 0.6001\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 933s 84s/step - loss: 92.2573 - accuracy: 0.6042 - auc: 0.5487 - val_loss: 92.0684 - val_accuracy: 0.6055 - val_auc: 0.5991\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 935s 84s/step - loss: 91.8050 - accuracy: 0.5495 - auc: 0.4870 - val_loss: 91.5158 - val_accuracy: 0.6055 - val_auc: 0.6238\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 933s 84s/step - loss: 91.3186 - accuracy: 0.5964 - auc: 0.5261 - val_loss: 91.0099 - val_accuracy: 0.6014 - val_auc: 0.6398\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 931s 84s/step - loss: 90.8434 - accuracy: 0.6016 - auc: 0.4044 - val_loss: 90.5201 - val_accuracy: 0.6055 - val_auc: 0.5895\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 930s 83s/step - loss: 90.3193 - accuracy: 0.5807 - auc: 0.4699 - val_loss: 90.0337 - val_accuracy: 0.6055 - val_auc: 0.6462\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 932s 84s/step - loss: 89.7912 - accuracy: 0.6172 - auc: 0.5480 - val_loss: 89.5310 - val_accuracy: 0.6055 - val_auc: 0.6493\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 931s 84s/step - loss: 89.3245 - accuracy: 0.6094 - auc: 0.5142 - val_loss: 89.0574 - val_accuracy: 0.6055 - val_auc: 0.6323\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 931s 83s/step - loss: 88.8509 - accuracy: 0.5781 - auc: 0.5137 - val_loss: 88.5936 - val_accuracy: 0.6055 - val_auc: 0.6061\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 931s 84s/step - loss: 88.3540 - accuracy: 0.6745 - auc: 0.4830 - val_loss: 88.1027 - val_accuracy: 0.6055 - val_auc: 0.6245\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 931s 83s/step - loss: 87.8764 - accuracy: 0.6510 - auc: 0.4623 - val_loss: 87.7030 - val_accuracy: 0.6055 - val_auc: 0.6049\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 931s 84s/step - loss: 87.4077 - accuracy: 0.6328 - auc: 0.4883 - val_loss: 87.1741 - val_accuracy: 0.6055 - val_auc: 0.5989\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 929s 83s/step - loss: 86.9291 - accuracy: 0.6667 - auc: 0.5477 - val_loss: 86.6928 - val_accuracy: 0.6051 - val_auc: 0.5921\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 931s 84s/step - loss: 86.5367 - accuracy: 0.5964 - auc: 0.4458 - val_loss: 86.2350 - val_accuracy: 0.6055 - val_auc: 0.6113\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 933s 84s/step - loss: 86.0788 - accuracy: 0.5052 - auc: 0.4471 - val_loss: 85.8405 - val_accuracy: 0.6055 - val_auc: 0.5713\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 931s 83s/step - loss: 85.6431 - accuracy: 0.5807 - auc: 0.4493 - val_loss: 85.3103 - val_accuracy: 0.6059 - val_auc: 0.5979\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 929s 83s/step - loss: 85.1447 - accuracy: 0.5831 - auc: 0.4853 - val_loss: 84.8662 - val_accuracy: 0.6055 - val_auc: 0.6237\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 931s 84s/step - loss: 84.6684 - accuracy: 0.6042 - auc: 0.4928 - val_loss: 84.4164 - val_accuracy: 0.6055 - val_auc: 0.5937\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 930s 83s/step - loss: 84.1781 - accuracy: 0.6797 - auc: 0.5057 - val_loss: 83.9510 - val_accuracy: 0.6055 - val_auc: 0.5859\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 931s 83s/step - loss: 83.7947 - accuracy: 0.5443 - auc: 0.4723 - val_loss: 83.5213 - val_accuracy: 0.6055 - val_auc: 0.6218\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 930s 83s/step - loss: 83.3038 - accuracy: 0.6562 - auc: 0.4904 - val_loss: 83.0597 - val_accuracy: 0.6055 - val_auc: 0.6053\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 930s 83s/step - loss: 82.8689 - accuracy: 0.6380 - auc: 0.5453 - val_loss: 82.6475 - val_accuracy: 0.6055 - val_auc: 0.5956\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 929s 83s/step - loss: 82.4193 - accuracy: 0.6094 - auc: 0.5040 - val_loss: 82.2246 - val_accuracy: 0.6055 - val_auc: 0.5856\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 931s 84s/step - loss: 82.0197 - accuracy: 0.6016 - auc: 0.4867 - val_loss: 81.7469 - val_accuracy: 0.6055 - val_auc: 0.6117\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 962s 86s/step - loss: 81.5646 - accuracy: 0.6406 - auc: 0.5053 - val_loss: 81.3447 - val_accuracy: 0.6055 - val_auc: 0.5904\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 946s 85s/step - loss: 81.1319 - accuracy: 0.6042 - auc: 0.4977 - val_loss: 80.8942 - val_accuracy: 0.6055 - val_auc: 0.6097\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 935s 84s/step - loss: 80.7213 - accuracy: 0.5859 - auc: 0.4967 - val_loss: 80.4553 - val_accuracy: 0.6055 - val_auc: 0.5767\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 935s 84s/step - loss: 80.2696 - accuracy: 0.6589 - auc: 0.5528 - val_loss: 80.0321 - val_accuracy: 0.6055 - val_auc: 0.5484\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 935s 84s/step - loss: 79.8228 - accuracy: 0.6745 - auc: 0.4762 - val_loss: 79.6345 - val_accuracy: 0.6055 - val_auc: 0.6097\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 936s 84s/step - loss: 79.4281 - accuracy: 0.6276 - auc: 0.4943 - val_loss: 79.1898 - val_accuracy: 0.6055 - val_auc: 0.5549\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 936s 84s/step - loss: 79.0266 - accuracy: 0.6120 - auc: 0.4665 - val_loss: 78.7726 - val_accuracy: 0.6055 - val_auc: 0.5342\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 936s 84s/step - loss: 78.5963 - accuracy: 0.6069 - auc: 0.4878 - val_loss: 78.4338 - val_accuracy: 0.6055 - val_auc: 0.4909\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 938s 84s/step - loss: 78.1954 - accuracy: 0.6146 - auc: 0.4814 - val_loss: 77.9657 - val_accuracy: 0.6055 - val_auc: 0.4906\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 938s 84s/step - loss: 77.7561 - accuracy: 0.6615 - auc: 0.5089 - val_loss: 77.5399 - val_accuracy: 0.6055 - val_auc: 0.5218\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 939s 84s/step - loss: 77.3874 - accuracy: 0.5781 - auc: 0.4847 - val_loss: 77.1323 - val_accuracy: 0.6055 - val_auc: 0.5105\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 939s 84s/step - loss: 76.9428 - accuracy: 0.6328 - auc: 0.4936 - val_loss: 76.7250 - val_accuracy: 0.6055 - val_auc: 0.5470\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 944s 85s/step - loss: 76.5404 - accuracy: 0.6328 - auc: 0.4915 - val_loss: 76.3284 - val_accuracy: 0.6055 - val_auc: 0.5524\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 957s 86s/step - loss: 76.1489 - accuracy: 0.6432 - auc: 0.4369 - val_loss: 75.9183 - val_accuracy: 0.6055 - val_auc: 0.5108\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 982s 88s/step - loss: 75.7509 - accuracy: 0.6068 - auc: 0.5351 - val_loss: 75.5201 - val_accuracy: 0.6055 - val_auc: 0.5851\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 941s 84s/step - loss: 75.4016 - accuracy: 0.5417 - auc: 0.4503 - val_loss: 75.1251 - val_accuracy: 0.6055 - val_auc: 0.5283\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 943s 85s/step - loss: 74.9572 - accuracy: 0.6068 - auc: 0.4956 - val_loss: 74.7528 - val_accuracy: 0.6055 - val_auc: 0.5082\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 972s 87s/step - loss: 74.5805 - accuracy: 0.5833 - auc: 0.4858 - val_loss: 74.4077 - val_accuracy: 0.6055 - val_auc: 0.4483\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 946s 85s/step - loss: 74.1503 - accuracy: 0.6562 - auc: 0.5230 - val_loss: 73.9517 - val_accuracy: 0.6055 - val_auc: 0.4864\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 944s 85s/step - loss: 73.7737 - accuracy: 0.6406 - auc: 0.4923 - val_loss: 73.5657 - val_accuracy: 0.6055 - val_auc: 0.5572\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 944s 85s/step - loss: 73.4159 - accuracy: 0.6172 - auc: 0.4706 - val_loss: 73.2137 - val_accuracy: 0.6055 - val_auc: 0.5429\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 943s 85s/step - loss: 73.0355 - accuracy: 0.5729 - auc: 0.4851 - val_loss: 72.7981 - val_accuracy: 0.6055 - val_auc: 0.5237\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 941s 84s/step - loss: 72.5974 - accuracy: 0.6649 - auc: 0.5248 - val_loss: 72.4660 - val_accuracy: 0.6055 - val_auc: 0.5548\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 965s 87s/step - loss: 72.2602 - accuracy: 0.6198 - auc: 0.4816 - val_loss: 72.0405 - val_accuracy: 0.6055 - val_auc: 0.5356\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 961s 86s/step - loss: 71.8831 - accuracy: 0.6068 - auc: 0.4862 - val_loss: 71.6712 - val_accuracy: 0.6055 - val_auc: 0.5412\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 944s 85s/step - loss: 71.4832 - accuracy: 0.6510 - auc: 0.5142 - val_loss: 71.2952 - val_accuracy: 0.6055 - val_auc: 0.5563\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 943s 85s/step - loss: 71.1714 - accuracy: 0.5729 - auc: 0.4355 - val_loss: 70.9192 - val_accuracy: 0.6055 - val_auc: 0.5664\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 951s 85s/step - loss: 70.7487 - accuracy: 0.6120 - auc: 0.5293 - val_loss: 70.5521 - val_accuracy: 0.6055 - val_auc: 0.5541\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 973s 87s/step - loss: 70.3919 - accuracy: 0.6042 - auc: 0.5297 - val_loss: 70.1844 - val_accuracy: 0.6055 - val_auc: 0.5446\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 944s 85s/step - loss: 70.0217 - accuracy: 0.6120 - auc: 0.4728 - val_loss: 69.8730 - val_accuracy: 0.6055 - val_auc: 0.5175\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 942s 85s/step - loss: 69.6424 - accuracy: 0.6432 - auc: 0.4996 - val_loss: 69.4593 - val_accuracy: 0.6055 - val_auc: 0.5291\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 942s 84s/step - loss: 69.3128 - accuracy: 0.6068 - auc: 0.4667 - val_loss: 69.0990 - val_accuracy: 0.6055 - val_auc: 0.5486\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 945s 85s/step - loss: 68.9678 - accuracy: 0.5781 - auc: 0.4537 - val_loss: 68.7532 - val_accuracy: 0.6055 - val_auc: 0.5694\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 942s 84s/step - loss: 68.5715 - accuracy: 0.6302 - auc: 0.5032 - val_loss: 68.4385 - val_accuracy: 0.6055 - val_auc: 0.5087\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 941s 84s/step - loss: 68.2008 - accuracy: 0.6823 - auc: 0.4953 - val_loss: 68.0276 - val_accuracy: 0.6055 - val_auc: 0.4959\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 941s 84s/step - loss: 67.8341 - accuracy: 0.6719 - auc: 0.5355 - val_loss: 67.6904 - val_accuracy: 0.6055 - val_auc: 0.5337\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 943s 85s/step - loss: 67.5330 - accuracy: 0.5833 - auc: 0.5165 - val_loss: 67.3216 - val_accuracy: 0.6055 - val_auc: 0.5862\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 942s 85s/step - loss: 67.1703 - accuracy: 0.6095 - auc: 0.5280 - val_loss: 66.9764 - val_accuracy: 0.6055 - val_auc: 0.5201\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 944s 85s/step - loss: 66.7944 - accuracy: 0.6641 - auc: 0.4974 - val_loss: 66.6765 - val_accuracy: 0.6055 - val_auc: 0.5098\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 944s 85s/step - loss: 66.4577 - accuracy: 0.6615 - auc: 0.4925 - val_loss: 66.2842 - val_accuracy: 0.6055 - val_auc: 0.5362\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 948s 85s/step - loss: 66.1227 - accuracy: 0.6302 - auc: 0.5317 - val_loss: 65.9417 - val_accuracy: 0.6055 - val_auc: 0.5466\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 942s 84s/step - loss: 65.8020 - accuracy: 0.5964 - auc: 0.5167 - val_loss: 65.6482 - val_accuracy: 0.6055 - val_auc: 0.5272\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 943s 85s/step - loss: 65.4724 - accuracy: 0.6094 - auc: 0.4584 - val_loss: 65.2940 - val_accuracy: 0.6055 - val_auc: 0.5105\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 945s 85s/step - loss: 65.1244 - accuracy: 0.6250 - auc: 0.4959 - val_loss: 64.9314 - val_accuracy: 0.6055 - val_auc: 0.4974\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 955s 86s/step - loss: 64.7971 - accuracy: 0.6068 - auc: 0.4909 - val_loss: 64.5993 - val_accuracy: 0.6055 - val_auc: 0.5016\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 947s 85s/step - loss: 64.4686 - accuracy: 0.6146 - auc: 0.4794 - val_loss: 64.3093 - val_accuracy: 0.6055 - val_auc: 0.5243\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 942s 85s/step - loss: 64.1588 - accuracy: 0.5807 - auc: 0.4313 - val_loss: 64.0082 - val_accuracy: 0.6055 - val_auc: 0.5083\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 942s 84s/step - loss: 63.7908 - accuracy: 0.6354 - auc: 0.4913 - val_loss: 63.6490 - val_accuracy: 0.6055 - val_auc: 0.4970\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 940s 84s/step - loss: 63.4786 - accuracy: 0.6094 - auc: 0.5049 - val_loss: 63.2935 - val_accuracy: 0.6055 - val_auc: 0.5258\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 938s 84s/step - loss: 63.1333 - accuracy: 0.6276 - auc: 0.4861 - val_loss: 62.9875 - val_accuracy: 0.6055 - val_auc: 0.5489\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 939s 84s/step - loss: 62.8456 - accuracy: 0.5781 - auc: 0.4706 - val_loss: 62.6339 - val_accuracy: 0.6055 - val_auc: 0.5041\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 939s 84s/step - loss: 62.4995 - accuracy: 0.6250 - auc: 0.5295 - val_loss: 62.3140 - val_accuracy: 0.6055 - val_auc: 0.5698\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 937s 84s/step - loss: 62.1734 - accuracy: 0.6253 - auc: 0.5143 - val_loss: 61.9968 - val_accuracy: 0.6055 - val_auc: 0.5494\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 939s 84s/step - loss: 61.8432 - accuracy: 0.6432 - auc: 0.5309 - val_loss: 61.7375 - val_accuracy: 0.6055 - val_auc: 0.5451\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 938s 84s/step - loss: 61.5115 - accuracy: 0.6771 - auc: 0.5328 - val_loss: 61.3650 - val_accuracy: 0.6055 - val_auc: 0.6218\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 937s 84s/step - loss: 61.2588 - accuracy: 0.5833 - auc: 0.4681 - val_loss: 61.0562 - val_accuracy: 0.6055 - val_auc: 0.5636\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 938s 84s/step - loss: 60.9342 - accuracy: 0.6406 - auc: 0.4714 - val_loss: 60.7436 - val_accuracy: 0.6055 - val_auc: 0.5862\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 938s 84s/step - loss: 60.6302 - accuracy: 0.5677 - auc: 0.4902 - val_loss: 60.4429 - val_accuracy: 0.6055 - val_auc: 0.5731\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 937s 84s/step - loss: 60.2891 - accuracy: 0.6510 - auc: 0.4955 - val_loss: 60.1311 - val_accuracy: 0.6055 - val_auc: 0.5786\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 938s 84s/step - loss: 59.9908 - accuracy: 0.6094 - auc: 0.5303 - val_loss: 59.8512 - val_accuracy: 0.6055 - val_auc: 0.6143\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 940s 84s/step - loss: 59.7075 - accuracy: 0.6198 - auc: 0.4539 - val_loss: 59.5469 - val_accuracy: 0.6055 - val_auc: 0.5110\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 939s 84s/step - loss: 59.3719 - accuracy: 0.6276 - auc: 0.5220 - val_loss: 59.2643 - val_accuracy: 0.6055 - val_auc: 0.4978\n",
      "Wall time: 1d 2h 13min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "checkpoint = ModelCheckpoint('model5.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    [train_input, img_feature], train_labels,\n",
    "    validation_split=0.3,\n",
    "    epochs=100 ,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=32,\n",
    "    steps_per_epoch=12   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f7660018a1f48e52e1285a6cd46b08bf9e759a1146999e72bfa769101423a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
